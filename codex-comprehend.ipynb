{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d5b24b-61a8-4a58-8f81-53c1874259e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OEIS Scraping\n",
    "\n",
    "OEIS lists a number of integer sequences, including their descriptions in English, and some sequences also list a small sample of code in various languages that either yields specific elements of that sequence or the sequence itself. By prompting GPT with certain samples of OEIS sequence descriptions and their corresponding code, we can test whether or not GPT understands the code snippets by giving it a new code sample and seeing if it will be able to extend it with a valid description of what that code does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431b4433-2f14-4ee4-82c1-b3cfe315fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to scrape dataset (depending on internet speed may take a while - will probably refactor later if needed)\n",
    "# from oeis_scraper import *\n",
    "# scrape('oeis_data.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a45bcd6-43e2-408f-bace-550346868973",
   "metadata": {},
   "source": [
    "For the purposes of keeping the code as readable as possible, only sequences with Python code were collected, based on Python's reputation for having very simple code that is easy for humans to read, and because it was part of the datasets that GPT-Codex was trained on due to its considerable popularity. Out of the many sequences in the OEIS database, there were only 6,777 sequences that contained Python code. Also, because I am not a mathematician and because many of the sequences are actually named after the mathematicians who discovered them, a smaller subset of about 80 of those sequences were selected for manual review to ensure that GPT is able to generate adequate descriptions. These sequences were chosen due to their relatively simple nature that would not require more complex knowledge of number theory: If the model were to generate a high level description of what the code does or what numbers are produced, I would be able to confirm that the code indeed does do what GPT says it does, whereas something like the Kolakoski sequence would be something that I did not know and would not be able to verify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268419c9-1356-4b03-8b9c-23ef3c5bbd6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code obfuscation\n",
    "Once we have the description of integer sequences and the code samples, we need to do some preprocessing first; because GPT-Codex is trained on a large corpus of data that is scraped from the internet, it is likely that something like OEIS would be something it has seen before. In fact, testing GPT with unmodified code samples yields descriptions that are lifted verbatim from OEIS. In order to get around this, we need to obfuscate the code in such a way that it is still fairly understandable (at least from a human perspective) but that functionally accomplishes the same thing as the original code sample.\n",
    "\n",
    "Given that many of the code samples in the dataset are from high-level mathematical backgrounds, it makes sense that they would be more functional than imperative in terms of the programming style used. As such, high-level map, filter, and reduce methods are an ideal target to modify, given that they can easily be converted to and from imperative loops. List and set comprehensions are also good targets because they are also fairly simple to rewrite. To stretch out limited data and to see what combinations of changes are optimal for GPT, we can then chain different obfuscations in different orders. Full specifications of what I changed and how are listed in the obfuscate_data.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69bb24a8-da66-4fc5-886d-6b98594189c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obfuscate_data import *\n",
    "# obfuscate('oeis_data.jsonl', 'oeis_data_obfuscated.jsonl')\n",
    "obfuscate('google-python-data/mbpp.jsonl', 'google-python-data/mbpp_obfuscated.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d719b930-0e6f-4efa-ac73-59990c48e926",
   "metadata": {},
   "source": [
    "## GPT-Codex processing\n",
    "Now, we process the code samples through GPT-Codex. Currently, there are 5 obfuscations being applied one after the next with intermediate results being saved. Also, there's different settings that we can set GPT to, including sampling temperature and penalties for repeating words. For now, we'll experiment with penalizing repetitions based on presence and frequency with value set to either 1 or 0(see API for details), so that's 4 more combinations of settings for a total of 20 different GPT calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c32a43-3152-470b-bfaf-894c81bc2f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmc/.conda/envs/codex-comprehension/lib/python3.10/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/bmc/.conda/envs/codex-comprehension/lib/python3.10/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/bmc/.conda/envs/codex-comprehension/lib/python3.10/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from gpt_codex_query import *\n",
    "\n",
    "# indices.txt has the sequence numbers for 70 sequences that I can understand at a glance\n",
    "# with open('indices.txt') as indices:\n",
    "#    sequences = set(line.strip() for line in indices)\n",
    "\n",
    "# get readable sequences\n",
    "# write_completions('codex-completions-subset.jsonl', read_snippets('oeis_data_obfuscated.jsonl', sequences))\n",
    "    \n",
    "# get all sequences\n",
    "# write_completions('codex-completions-full.jsonl', read_snippets('oeis_data_obfuscated.jsonl'))\n",
    "write_completions('google-python-data/mbpp_completions.jsonl', read_snippets('google-python-data/mbpp_obfuscated.jsonl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
